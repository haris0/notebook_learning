{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_Crawling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TwfP7NeG8mA",
        "colab_type": "text"
      },
      "source": [
        "# **1. Installing Library**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQxbVxjk-Dy8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a312f155-f4ad-4903-89f6-95ff1654a8ca"
      },
      "source": [
        "!pip install twint"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twint\n",
            "  Downloading https://files.pythonhosted.org/packages/69/e1/4daa62fbae8a34558015c227a8274bb2598e0fc6e330bdeb8484ed154ce7/twint-2.1.20.tar.gz\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/39/7eb5f98d24904e0f6d3edb505d4aa60e3ef83c0a58d6fe18244a51757247/aiohttp-3.6.2-cp36-cp36m-manylinux1_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.0MB/s \n",
            "\u001b[?25hCollecting aiodns\n",
            "  Downloading https://files.pythonhosted.org/packages/da/01/8f2d49b441573fd2478833bdba91cf0b853b4c750a1fbb9e98de1b94bb22/aiodns-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from twint) (4.6.3)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/c5/7e1a0d7b4afd83d6f8de794fce82820ec4c5136c6d52e14000822681a842/cchardet-2.1.6-cp36-cp36m-manylinux2010_x86_64.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 13.0MB/s \n",
            "\u001b[?25hCollecting elasticsearch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/4e/8d4190b41c72ee04b03c2404c7ac39e671a95616a6e6a4260d2098f3c7b2/elasticsearch-7.8.0-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 13.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pysocks in /usr/local/lib/python3.6/dist-packages (from twint) (1.7.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from twint) (1.0.5)\n",
            "Collecting aiohttp_socks\n",
            "  Downloading https://files.pythonhosted.org/packages/06/28/67fcf0de9046f98052a30e0d8dc2529d8662b18f78b92eea42d7048a5e80/aiohttp_socks-0.3.9-py3-none-any.whl\n",
            "Collecting schedule\n",
            "  Downloading https://files.pythonhosted.org/packages/57/22/3a709462eb02412bd1145f6e53604f36bba191e3e4e397bea4a718fec38c/schedule-0.6.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: geopy in /usr/local/lib/python3.6/dist-packages (from twint) (1.17.0)\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Collecting googletransx\n",
            "  Downloading https://files.pythonhosted.org/packages/27/e1/77cd530afec7944d40c5bdd260bcc111be4012b045c82d4e3ffec90b2a42/googletransx-2.4.2.tar.gz\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (3.6.6)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting multidict<5.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/95/f50352b5366e7d579e8b99631680a9e32e1b22adfa1629a8f23b1d22d5e2/multidict-4.7.6-cp36-cp36m-manylinux1_x86_64.whl (148kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 14.8MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/8f/0209fc5d975f839344c33c822ff2f7ef80f6b1e984673a5a68f960bfa583/yarl-1.4.2-cp36-cp36m-manylinux1_x86_64.whl (252kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 16.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->twint) (19.3.0)\n",
            "Collecting pycares>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/2d/7f4984a23f6e99cf6a8b20ddc59308efb209fe81e79c97af65e9b30eefae/pycares-3.1.1-cp36-cp36m-manylinux2010_x86_64.whl (228kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 12.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiodns->twint) (3.6.6)\n",
            "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (1.24.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch->twint) (2020.4.5.2)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas->twint) (1.18.5)\n",
            "Requirement already satisfied: geographiclib<2,>=1.49 in /usr/local/lib/python3.6/dist-packages (from geopy->twint) (1.50)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from googletransx->twint) (2.23.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp->twint) (2.9)\n",
            "Requirement already satisfied: cffi>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pycares>=3.0.0->aiodns->twint) (1.14.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->twint) (1.12.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.5.0->pycares>=3.0.0->aiodns->twint) (2.20)\n",
            "Building wheels for collected packages: twint, fake-useragent, googletransx, idna-ssl\n",
            "  Building wheel for twint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twint: filename=twint-2.1.20-cp36-none-any.whl size=33921 sha256=a05da9aa6bb343aae716e99fce1a50684450414b8ab0e5bda288483508cd1f00\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/25/c7/855c1d896087ef84df6e6713b8adb073aff99af119450e1e4f\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=52ea8ddaf4f6b6baf0b11db6b44b16ecd6e65eb934f75ab2c714d1d96a4ff2e3\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for googletransx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletransx: filename=googletransx-2.4.2-cp36-none-any.whl size=15969 sha256=cf426366459c5e854e001a8bef0d7177029ed3f7d6dcc7dd4a2e2900e4848332\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/63/5f/75e7e94eb62517946116a783e4cd8970c4789c990bbc732616\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3162 sha256=2dcf5fad8509feb509e21697fcf04791076aa244fa388d05470ba1b307b4ffb6\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built twint fake-useragent googletransx idna-ssl\n",
            "Installing collected packages: idna-ssl, multidict, async-timeout, yarl, aiohttp, pycares, aiodns, cchardet, elasticsearch, aiohttp-socks, schedule, fake-useragent, googletransx, twint\n",
            "Successfully installed aiodns-2.0.0 aiohttp-3.6.2 aiohttp-socks-0.3.9 async-timeout-3.0.1 cchardet-2.1.6 elasticsearch-7.8.0 fake-useragent-0.1.11 googletransx-2.4.2 idna-ssl-1.1.0 multidict-4.7.6 pycares-3.1.1 schedule-0.6.0 twint-2.1.20 yarl-1.4.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcDh8-pcDPo3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "6f740477-b8f0-4c23-f4c6-8e122efae02b"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/a2/e5f28b67b7d883c9a6585c0ef32b4bb002bff0292b3008f3d6d3fc7eee59/XlsxWriter-1.2.9-py2.py3-none-any.whl (141kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 15.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spaIfLn_HHm1",
        "colab_type": "text"
      },
      "source": [
        "# **2. Define Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La2edmMY_5P1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import twint\n",
        "import pandas as pd\n",
        "import time\n",
        "from time import sleep\n",
        "import asyncio\n",
        "from datetime import datetime, timedelta\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMUlCJWTcelr",
        "colab_type": "text"
      },
      "source": [
        "## Fungsi Konversi Detik\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_ZjowrrfzGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display time more elegantly\n",
        "intervals = (\n",
        "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
        "    ('days', 86400),    # 60 * 60 * 24\n",
        "    ('hours', 3600),    # 60 * 60\n",
        "    ('minutes', 60),\n",
        "    ('seconds', 1),\n",
        "    )\n",
        "\n",
        "def display_time(seconds, granularity=2):\n",
        "    result = []\n",
        "\n",
        "    for name, count in intervals:\n",
        "        value = seconds // count\n",
        "        if value:\n",
        "            seconds -= value * count\n",
        "            if value == 1:\n",
        "                name = name.rstrip('s')\n",
        "            result.append(\"{} {}\".format(value, name))\n",
        "    return ', '.join(result[:granularity])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekt1I0EqclOl",
        "colab_type": "text"
      },
      "source": [
        "## Fungsi Crowling Berdasarkan tanggal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScK-lDgp_-Sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def by_date(query, since, until):\n",
        "  query = query\n",
        "\n",
        "  start_str = since\n",
        "  end_str = until\n",
        "  start_date = pd.to_datetime(start_str, format='%Y-%m-%d', errors='ignore')\n",
        "  end_date = pd.to_datetime(end_str, format='%Y-%m-%d', errors='ignore')\n",
        "  data_folder = \"/content/\" #Directory di sesuaikan dengan kebutuhan (Bisa di arahkan ke gdrive jika di run di colab)\n",
        "  filename = f\"{data_folder}{query}_tweets_{start_str}_{end_str}.csv\"\n",
        "  resume_file = f\"{data_folder}resume.txt\"\n",
        "\n",
        "  c = twint.Config()\n",
        "  c.Hide_output = True\n",
        "  c.Store_csv = True\n",
        "  c.Filter_retweets = True\n",
        "  c.Custom[\"tweet\"] = ['id', 'date', 'time','tweet','username']\n",
        "  c.Output = filename\n",
        "  c.Resume = resume_file\n",
        "  c.Search = query\n",
        "  c.Lang = 'id'\n",
        "\n",
        "  while start_date < end_date:\n",
        "\n",
        "      check = 0\n",
        "      c.Since = datetime.strftime(start_date, format='%Y-%m-%d')\n",
        "      c.Until = datetime.strftime(start_date + timedelta(days=1), format='%Y-%m-%d')\n",
        "\n",
        "      while check < 1:\n",
        "          try:\n",
        "              print(\"Running Search: Check \", start_date)\n",
        "              twint.run.Search(c)\n",
        "              check += 1\n",
        "\n",
        "          except Exception as e:\n",
        "              print(e, \"Sleeping for 7 mins\")\n",
        "              print(\"Check: \", check)\n",
        "              sleep(420)\n",
        "\n",
        "      os.remove(resume_file)\n",
        "\n",
        "      start_date = start_date + timedelta(days=1)\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tm3kELIhc4_7",
        "colab_type": "text"
      },
      "source": [
        "## Fungsi Crawling Berdasarkan Limit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nP-nzhfPc3uR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def by_limit(query, limit):\n",
        "  query = query\n",
        "\n",
        "  data_folder = \"/content/\" #Directory di sesuaikan dengan kebutuhan (Bisa di arahkan ke gdrive jika di run di colab)\n",
        "  filename = f\"{data_folder}{query}_tweets_{limit}.csv\"\n",
        "  resume_file = f\"{data_folder}resume.txt\"\n",
        "\n",
        "  c = twint.Config()\n",
        "  c.Hide_output = True\n",
        "  c.Store_csv = True\n",
        "  c.Filter_retweets = True\n",
        "  c.Search = query\n",
        "  c.Limit = limit\n",
        "  c.Custom[\"tweet\"] = ['id', 'date', 'time','tweet','username']\n",
        "  c.Output = filename\n",
        "  c.Resume = resume_file\n",
        "  c.Lang = 'id'\n",
        "\n",
        "  check = 0\n",
        "  while check < 1:\n",
        "      try:\n",
        "          twint.run.Search(c)\n",
        "          check += 1\n",
        "\n",
        "      except Exception as e:\n",
        "          print(e, \"Sleeping for 7 mins\")\n",
        "          print(\"Check: \", check)\n",
        "          sleep(420)\n",
        "\n",
        "  os.remove(resume_file)\n",
        "\n",
        "  return filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfg9-sUbc--k",
        "colab_type": "text"
      },
      "source": [
        "## Fungsi Cleaning Tweet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHdk7Lk0Hm5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import lxml\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "def tweet_cleaner(text):\n",
        "  tok = WordPunctTokenizer()\n",
        "  pat1 = r'@[A-Za-z0-9]+'\n",
        "  pat2 = r'\\w+:\\/\\/\\S+'\n",
        "  pat3 = r'\\S*twitter.com\\S*' \n",
        "  combined_pat = r'|'.join((pat1, pat2, pat3))\n",
        "  soup = BeautifulSoup(text, 'lxml')\n",
        "  souped = soup.get_text()\n",
        "  stripped = re.sub(combined_pat, '', souped)\n",
        "  try:\n",
        "    clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "  except:\n",
        "    clean = stripped\n",
        "  letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", clean)\n",
        "  lower_case = letters_only.lower()\n",
        "  words = tok.tokenize(lower_case)\n",
        "  return (\" \".join(words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Otl4Oc6sfcg9",
        "colab_type": "text"
      },
      "source": [
        "## Gabungkan Semua Jadi satu fungsi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56W-t33OI6Na",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crawling_by_date(query, since, until, clean=False):\n",
        "  \"\"\"\n",
        "  Dapat menggunakan lebih dari satu kata kunci \n",
        "  contoh \"BNI OR Nasabah OR ATM\"\n",
        "  contoh \"BNI AND ATM\"\n",
        "\n",
        "  Param since dan until formatnya :\n",
        "  \"YYYY-MM-DD\"\n",
        "  contoh : \"2020-06-01\"\n",
        "\n",
        "  Param clean secara default False,\n",
        "  set True jika ingin tweet yang di hasilkan dibersihkan\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  result_path = by_date(query, since, until)\n",
        "  df = pd.read_csv(result_path)\n",
        "  df = df.astype(str)\n",
        "  if clean:\n",
        "    df['tweet'] = df['tweet'].apply(lambda text: tweet_cleaner(text))\n",
        "  writer = pd.ExcelWriter(query+\"_tweet_\"+since+\"_\"+until+\".xlsx\", engine = 'xlsxwriter')\n",
        "  df.to_excel(writer, index=False)\n",
        "  writer.save()\n",
        "  writer.close()\n",
        "  os.remove(result_path)\n",
        "  print(\"Exe Time : \", display_time(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAYh5GrmKgkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crawling_by_limit(query, limit, clean=False):\n",
        "  \"\"\"\n",
        "  Dapat menggunakan lebih dari satu kata kunci \n",
        "  contoh \"BNI OR Nasabah OR ATM\"\n",
        "  contoh \"BNI AND ATM\"\n",
        "\n",
        "  Limit berupa Angka\n",
        "\n",
        "  Param clean secara default False,\n",
        "  set True jika ingin tweet yang di hasilkan dibersihkan\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  result_path = by_limit(query, limit)\n",
        "  df = pd.read_csv(result_path)\n",
        "  df = df.astype(str)\n",
        "  if clean:\n",
        "    df['tweet'] = df['tweet'].apply(lambda text: tweet_cleaner(text))\n",
        "  writer = pd.ExcelWriter(query+\"_tweet_\"+str(limit)+\".xlsx\", engine = 'xlsxwriter')\n",
        "  df.to_excel(writer, index=False)\n",
        "  writer.save()\n",
        "  writer.close()\n",
        "  os.remove(result_path)\n",
        "  print(\"Exe Time : \", display_time(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xsg3c8UbHQXH",
        "colab_type": "text"
      },
      "source": [
        "# **3. Execute Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OWlCeoGRM0-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "dd67de4b-41a3-4c59-b81e-16d15a048e5f"
      },
      "source": [
        "crawling_by_date('bni','2020-05-01', '2020-05-02', clean=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Search: Check  2020-05-01 00:00:00\n",
            "Exe Time :  54.0 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FF5LHIAaA-ee",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cf0ea2fc-c2e9-4019-9ff7-116fe29c0512"
      },
      "source": [
        "crawling_by_limit('bni', 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exe Time :  4.0 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0xzzNBhp16B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}