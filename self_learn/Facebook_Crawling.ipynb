{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Facebook_Crawling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdLZzRzcED2",
        "colab_type": "text"
      },
      "source": [
        "# **1. Install Lib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYpBt6bG3gyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ac155aa8-41fd-493c-d47b-60f3599d34ff"
      },
      "source": [
        "!pip install facebook-scraper"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting facebook-scraper\n",
            "  Downloading https://files.pythonhosted.org/packages/64/86/481a196ae84941a34a2172be709c61f12c528ae952995cdedf2d348f47ab/facebook_scraper-0.2.3-py3-none-any.whl\n",
            "Collecting html2text<2021.0.0,>=2020.1.16\n",
            "  Downloading https://files.pythonhosted.org/packages/ae/88/14655f727f66b3e3199f4467bafcc88283e6c31b562686bf606264e09181/html2text-2020.1.16-py3-none-any.whl\n",
            "Collecting requests-html<0.11.0,>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl\n",
            "Collecting fake-useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.23.0)\n",
            "Collecting pyquery\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from requests-html<0.11.0,>=0.10.0->facebook-scraper) (0.0.1)\n",
            "Collecting pyppeteer>=0.0.14\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/4b/3c2aabdd1b91fa52aa9de6cde33b488b0592b4d48efb0ad9efbf71c49f5b/pyppeteer-0.2.2-py3-none-any.whl (145kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 7.0MB/s \n",
            "\u001b[?25hCollecting parse\n",
            "  Downloading https://files.pythonhosted.org/packages/f4/65/220bb4075fddb09d5b3ea2c1c1fa66c1c72be9361ec187aab50fa161e576/parse-1.15.0.tar.gz\n",
            "Collecting w3lib\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/59/b6b14521090e7f42669cafdb84b0ab89301a42f1f1a82fcf5856661ea3a7/w3lib-1.22.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->requests-html<0.11.0,>=0.10.0->facebook-scraper) (3.0.4)\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=2.1 in /usr/local/lib/python3.6/dist-packages (from pyquery->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.2.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->requests-html<0.11.0,>=0.10.0->facebook-scraper) (4.6.3)\n",
            "Collecting tqdm<5.0.0,>=4.42.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/62/7663894f67ac5a41a0d8812d78d9d2a9404124051885af9d77dc526fb399/tqdm-4.47.0-py2.py3-none-any.whl (66kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.3MB/s \n",
            "\u001b[?25hCollecting pyee<8.0.0,>=7.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/f6/28/1cedd44c27907f1507a28ff2d36fc6cdb981c9deff2fa288bc48a700c7c9/pyee-7.0.2-py2.py3-none-any.whl\n",
            "Collecting appdirs<2.0.0,>=1.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Collecting websockets<9.0,>=8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/d9/856af84843912e2853b1b6e898ac8b802989fcf9ecf8e8445a1da263bf3b/websockets-8.1-cp36-cp36m-manylinux2010_x86_64.whl (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from w3lib->requests-html<0.11.0,>=0.10.0->facebook-scraper) (1.12.0)\n",
            "Building wheels for collected packages: fake-useragent, parse\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13484 sha256=f7a97512c4d2dbb80acc9b4fd56f1129a38997e8632fabfed20218e3c6c3f433\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "  Building wheel for parse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for parse: filename=parse-1.15.0-cp36-none-any.whl size=23710 sha256=3ac63a63d7087a091870048e708e8ea310bf5ba83e3e7b0f655421bcd026103a\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/ee/c8/eced0759f09fc635398ab1b8e89c38549b28e5db7fd4a53ba5\n",
            "Successfully built fake-useragent parse\n",
            "\u001b[31mERROR: pyppeteer 0.2.2 has requirement urllib3<2.0.0,>=1.25.8, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html2text, fake-useragent, cssselect, pyquery, tqdm, pyee, appdirs, websockets, pyppeteer, parse, w3lib, requests-html, facebook-scraper\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed appdirs-1.4.4 cssselect-1.1.0 facebook-scraper-0.2.3 fake-useragent-0.1.11 html2text-2020.1.16 parse-1.15.0 pyee-7.0.2 pyppeteer-0.2.2 pyquery-1.4.1 requests-html-0.10.0 tqdm-4.47.0 w3lib-1.22.0 websockets-8.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOYFe2x9f4h2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "outputId": "be2f7dbc-0ba1-4ecd-c657-af17dbbca9a7"
      },
      "source": [
        "!pip install xlsxwriter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xlsxwriter\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c8/a2/e5f28b67b7d883c9a6585c0ef32b4bb002bff0292b3008f3d6d3fc7eee59/XlsxWriter-1.2.9-py2.py3-none-any.whl (141kB)\n",
            "\r\u001b[K     |██▎                             | 10kB 17.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: xlsxwriter\n",
            "Successfully installed xlsxwriter-1.2.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fLGQ9Ynb5J-",
        "colab_type": "text"
      },
      "source": [
        "# **2. Init Lib**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_uy3t7P3jfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from facebook_scraper import get_posts\n",
        "import pandas as pd\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5J1jPhsVcNXC",
        "colab_type": "text"
      },
      "source": [
        "# **3. Define Helper Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZpgaXoSYDEo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to display time more elegantly\n",
        "intervals = (\n",
        "    ('weeks', 604800),  # 60 * 60 * 24 * 7\n",
        "    ('days', 86400),    # 60 * 60 * 24\n",
        "    ('hours', 3600),    # 60 * 60\n",
        "    ('minutes', 60),\n",
        "    ('seconds', 1),\n",
        "    )\n",
        "\n",
        "def display_time(seconds, granularity=2):\n",
        "    result = []\n",
        "\n",
        "    for name, count in intervals:\n",
        "        value = seconds // count\n",
        "        if value:\n",
        "            seconds -= value * count\n",
        "            if value == 1:\n",
        "                name = name.rstrip('s')\n",
        "            result.append(\"{} {}\".format(value, name))\n",
        "    return ', '.join(result[:granularity])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9PpUc0Ab3WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import lxml\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "def text_cleaner(text):\n",
        "  tok = WordPunctTokenizer()\n",
        "  pat1 = r'@[A-Za-z0-9]+'\n",
        "  pat2 = r'\\w+:\\/\\/\\S+'\n",
        "  pat3 = r'\\S*twitter.com\\S*' \n",
        "  combined_pat = r'|'.join((pat1, pat2, pat3))\n",
        "  soup = BeautifulSoup(text, 'lxml')\n",
        "  souped = soup.get_text()\n",
        "  stripped = re.sub(combined_pat, '', souped)\n",
        "  try:\n",
        "    clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "  except:\n",
        "    clean = stripped\n",
        "  letters_only = re.sub(\"[^a-zA-Z0-9]\", \" \", clean)\n",
        "  lower_case = letters_only.lower()\n",
        "  words = tok.tokenize(lower_case)\n",
        "  return (\" \".join(words)).strip()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDPJ3y3ocXOX",
        "colab_type": "text"
      },
      "source": [
        "# **4. Define Crawling Function**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2H9pmQveJ1Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_excel(generator, id, clean):\n",
        "  data = []\n",
        "  for post in generator:\n",
        "    if post['text'] == '':\n",
        "      pass\n",
        "    else:\n",
        "      data.append(post)\n",
        "  df = pd.DataFrame(data)\n",
        "  df.drop(['post_text'], axis=1, inplace=True)\n",
        "\n",
        "  if clean:\n",
        "    df['text'] = df['text'].apply(lambda text: text_cleaner(text))\n",
        "\n",
        "  path = \"/content/\"\n",
        "  writer = pd.ExcelWriter(f\"{path}{id}.xlsx\", engine = 'xlsxwriter')\n",
        "  df.to_excel(writer, index=False)\n",
        "  writer.save()\n",
        "  writer.close()\n",
        "\n",
        "  print(f\"Crawling is complete! get: {df.shape[0]} datas\")\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5jKxt4nByBd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_group_post(group_id, pages=1, clean=False):\n",
        "  \"\"\"\n",
        "  - param 'group_id' diisi dengan id group yang di dapat dari url group\n",
        "    contoh https://www.facebook.com/groups/101812157281468\n",
        "    maka id groupnya = 101812157281468\n",
        "\n",
        "  - param 'pages' berisi angka, berapa halaman dari group tersebut yang\n",
        "    ingin di crawling\n",
        "\n",
        "  - param 'clean' secara default False, \n",
        "    set True untuk membersihkan hasil post\n",
        "  \"\"\"\n",
        "  start_time = time.time()\n",
        "  post_result = get_posts(group=group_id, pages=pages)\n",
        "  \n",
        "  generate_excel(post_result,group_id, clean)\n",
        "  print(\"Exe Time : \", display_time(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFdcWEC8cg_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_page_post(page_id, pages=1, clean=False):\n",
        "  \"\"\"\n",
        "  - param 'page_id' diisi dengan id page yang didapat dari url page\n",
        "    contoh https://www.facebook.com/BRIofficialpage/\n",
        "    maka id pagenya = BRIofficialpage\n",
        "\n",
        "  - param 'pages' berisi angka, berapa halaman dari page tersebut yang\n",
        "    ingin di crawling\n",
        "\n",
        "  - param 'clean' secara default False, \n",
        "    set True untuk membersihkan hasil post\n",
        "  \"\"\"\n",
        "\n",
        "  start_time = time.time()\n",
        "  post_result = get_posts(page_id, pages=pages)\n",
        "  \n",
        "  generate_excel(post_result, page_id, clean)\n",
        "  print(\"Exe Time : \", display_time(time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-m--gdAfFgb",
        "colab_type": "text"
      },
      "source": [
        "# **5. Run Code!!!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1EnUc1xJJVg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "752d5ef8-afe8-4f92-e39b-fd33139a8d2b"
      },
      "source": [
        "df = get_group_post('101812157281468', 2, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawling is complete! get: 31 datas\n",
            "Exe Time :  1.0 second\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xdVrGAJaU25",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9314664a-a5d1-4f99-a196-e0779cff3b0c"
      },
      "source": [
        "df = get_page_post('BRIofficialpage', 2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Crawling is complete! get: 6 datas\n",
            "Exe Time :  1.0 second\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwQ4IuxIdF4z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}